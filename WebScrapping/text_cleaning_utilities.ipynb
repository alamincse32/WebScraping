{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a798b39-b06f-4178-a2a7-374c1f1ade31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fca5c5fe-0fae-4537-8c86-2bafa070a44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /mmfs1/home/md.al.1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /mmfs1/home/md.al.1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# prepare stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')\n",
    "STOP_WORDS = sw\n",
    "# STOP_WORDS.extend(\n",
    "#     [',', '`', \"'m\", \"i'm\", \"'s\", \"'ve\", \"n't\", 'know', 'make', 'want', 'say', 'think', 'need', 'tell', 'go', 'thing',\n",
    "#      'get', 'help', 'really', 'try', 'even', 'time', 'also', 'ask', 'find', 'take', 'talk', 'lot', 'give', 'way', 'see',\n",
    "#      'sure', 'much', 'look', 'good', 'one', 'maybe', 'ed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b47de5c-4be1-4a33-a4a7-b71c924c3630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization of tokens\n",
    "\n",
    "def get_lemmatized_sentence(text, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    tokens = []\n",
    "    nlp = spacy.load(name='en_core_web_md')\n",
    "    for sent in text:\n",
    "        doc = nlp(sent)\n",
    "        next_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                next_text.append(token.lemma_)\n",
    "        final = \" \".join(next_text)\n",
    "        tokens.append(final)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98f89ebd-efc6-4208-b1bb-cfc9ef35b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build bigran and tri-gram form the lemmatized words\n",
    "\n",
    "def build_bigram_trigram_models(sentence, min_count=5, threshold=50):\n",
    "    bigram = gensim.models.Phrases(sentence, min_count=min_count, threshold=threshold)\n",
    "    trigram = gensim.models.Phrases(bigram[sentence], threshold=threshold)\n",
    "    bigram_models = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_models = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    return [bigram_models, trigram_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8083e46-da3c-4c9b-8154-a728e9ffd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get birgram-trigram tokens\n",
    "\n",
    "def get_bigram_trigram_words(texts, models):\n",
    "    bigram_trigram_tokens = []\n",
    "    for text in texts:\n",
    "        tokens = models[1][models[0][text]]\n",
    "        bigram_trigram_tokens.append(tokens)\n",
    "    return bigram_trigram_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8693c6f-6487-44f7-beab-34f22edf4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bigram words\n",
    "\n",
    "def get_bigram_words(texts, model):\n",
    "    bigram_tokens = []\n",
    "    for text in texts:\n",
    "        tokens = model[text]\n",
    "        bigram_tokens.append(tokens)\n",
    "    return bigram_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee2c02c-a815-433c-a14c-41e51e0ce65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple token process from nltk\n",
    "\n",
    "def get_clean_tokens(texts):\n",
    "    final_token = []\n",
    "    for text in texts:\n",
    "        tokens = simple_preprocess(text, deacc=True)\n",
    "        tokens = [token for token in tokens if token not in STOP_WORDS]\n",
    "        final_token.extend(tokens)\n",
    "    return final_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be4f978b-facb-423b-adf0-b199d83d9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence spliting from text\n",
    "\n",
    "def split_sentences(special_character_removed):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentence_tokenize = []\n",
    "    for text in special_character_removed:\n",
    "        a = split_into_sentences(sent_tokenizer, text.lower())\n",
    "        sentence_tokenize.extend(a)\n",
    "    return sentence_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e585c23-cdac-4da2-b7c7-dc3f985d5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call sentence spliting from the below function\n",
    "\n",
    "def split_into_sentences(sent_tokenizer, text):\n",
    "    return sent_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70e4f631-c5be-4df1-97f0-d77fc2c5883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove frequent tokens from the corpus\n",
    "\n",
    "def removed_frequent_words(texts):\n",
    "    id2words = corpora.Dictionary(texts)\n",
    "    corpus = [id2words.doc2bow(text) for text in texts]\n",
    "\n",
    "    # tf_idf = TfidfModel(corpus, id2word=id2words)\n",
    "    # low_value = 0.03\n",
    "    # words = []\n",
    "    # words_missing_in_tfidf = []\n",
    "    # for i in range(len(corpus)):\n",
    "    #     bow = corpus[i]\n",
    "    #     tfidf_ids = [id for id, value in tf_idf[bow]]\n",
    "    #     bow_ids = [id for id, value in bow]\n",
    "    #     low_value_words = [id for id, value in tf_idf[bow] if value < low_value]\n",
    "    #     drops = low_value_words + words_missing_in_tfidf\n",
    "    #     for item in drops:\n",
    "    #         words.append(id2words[item])\n",
    "    #     words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "    #\n",
    "    #     new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    #     corpus[i] = new_bow\n",
    "    return [corpus, id2words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b932f795-1b97-49fb-b073-f37f53dcad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emojis from the text\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f57e883c-b6cf-4831-be8c-26613e34c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters from the text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Newlines (replaced with space to preserve cases like word1\\nword2)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    # Remove resulting ' '\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "\n",
    "    # emails\n",
    "    text = re.sub('\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "    # > Quotes\n",
    "    text = re.sub(r'\\\"?\\\\?&?gt;?', '', text)\n",
    "\n",
    "    # Bullet points/asterisk (bold/italic)\n",
    "    text = re.sub(r'\\*', '', text)\n",
    "    text = re.sub('&amp;#x200B;', '', text)\n",
    "\n",
    "    # remove the [removed] from text\n",
    "    text = text.replace('[removed]', '')\n",
    "\n",
    "    # remove the [deleted] from text\n",
    "    text = text.replace('[deleted]', '')\n",
    "\n",
    "    # things in parantheses or brackets\n",
    "    text = re.sub(r'[\\[.*?\\]\\(.*?\\)]', '', text)\n",
    "\n",
    "    # remove URLS\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "\n",
    "    # Strikethrough\n",
    "    text = re.sub('~', '', text)\n",
    "    \n",
    "\n",
    "    # Exclamation mark remove\n",
    "    text = re.sub('!', '', text)\n",
    "\n",
    "    # Spoiler, which is used with < less-than (Preserves the text)\n",
    "    text = re.sub('&lt;', '', text)\n",
    "    text = re.sub(r'!(.*?)!', r'\\1', text)\n",
    "\n",
    "    # Code, inline and block\n",
    "    text = re.sub('`', '', text)\n",
    "\n",
    "    # Superscript (Preserves the text)\n",
    "    text = re.sub(r'\\^\\((.*?)\\)', r'\\1', text)\n",
    "\n",
    "    # Table\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(':-', '', text)\n",
    "\n",
    "    # Heading\n",
    "    text = re.sub('#', '', text)\n",
    "    \n",
    "    text = re.sub('\"', '', text)\n",
    "    \n",
    "    text = re.sub('\\'', '', text)\n",
    "    text = re.sub(',', '', text)\n",
    "    text = re.sub('`', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018738b1-4f29-41f0-bfbc-a7a85d34c119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3540dc9-a7d2-4bd9-ad07-01699d2b06de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46630498-93cc-403c-9eca-042087ae298d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
